{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd7d96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 12:50:31.717206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 12:50:33.332235: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-14 12:50:45.896844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/gmp/6.2.1/b1/lib:/software/glpk/4.65/lib:/software/zmq/4.2.3/b1/lib:/software/git/2.30.1/lib64:/software/gcc/7.3.0/lib64:/software/gcc/7.3.0/lib:/software/openmpi/4.0.4/b1/lib:/software/cuda/11.4/usr/local/cuda-11.4/lib64:/software/cuda/11.4/usr/local/cuda-11.4/targets/x86_64-linux/lib:/software/cudnn/11.4-8.2.4.15/lib64:/software/slurm/current/lib64:/software/slurm/current/lib\n",
      "2023-02-14 12:50:45.897131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/gmp/6.2.1/b1/lib:/software/glpk/4.65/lib:/software/zmq/4.2.3/b1/lib:/software/git/2.30.1/lib64:/software/gcc/7.3.0/lib64:/software/gcc/7.3.0/lib:/software/openmpi/4.0.4/b1/lib:/software/cuda/11.4/usr/local/cuda-11.4/lib64:/software/cuda/11.4/usr/local/cuda-11.4/targets/x86_64-linux/lib:/software/cudnn/11.4-8.2.4.15/lib64:/software/slurm/current/lib64:/software/slurm/current/lib\n",
      "2023-02-14 12:50:45.897141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b876cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNs_found = np.load('../saved_models/shp2-rnn/spy_RNs_sampling.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77530ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('../saved_models/shp2-rnn/spy_model.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('../saved_models/shp2-rnn/spy_model_weights.h5') \n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f82fb4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427, 190)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(RNs_found, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b751b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f023c3e6af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "14/14 [==============================] - 3s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "def counts_aa(vec):\n",
    "    counts =  tf.histogram_fixed_width(vec, [0, 20], nbins=21)[1:]\n",
    "    return counts /tf.reduce_sum(counts)\n",
    "\n",
    "y_hat_RN = loaded_model.predict([RNs_found, np.array([counts_aa(xi) for xi in RNs_found])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab4b65dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 16.,  22.,  15.,  11.,  14.,  21.,  21.,  31.,  46., 230.]),\n",
       " array([0.04418208, 0.13851126, 0.23284042, 0.32716957, 0.42149875,\n",
       "        0.5158279 , 0.6101571 , 0.70448625, 0.79881537, 0.89314455,\n",
       "        0.9874737 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_hat_RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2800eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(seq_vector):\n",
    "  alphabet = ['A','R','N','D','C','Q','E','G','H','I', 'L','K','M','F','P','S','T','W','Y','V']\n",
    "  seq = []\n",
    "  for i, index in enumerate(seq_vector.astype('int')):\n",
    "    if index == 0:\n",
    "      break\n",
    "    seq.append(alphabet[index-1])\n",
    "  seq = ''.join(seq)\n",
    "  return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0019d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_sorted = np.argsort(y_hat_RN,axis=0)\n",
    "# 5 most probable RN\n",
    "most_likely_RN =  RNs_found[indices_sorted][:5]\n",
    "# 5 least probable RN\n",
    "least_likely_RN =  RNs_found[indices_sorted][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33b96b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.04418209],\n",
       "       [0.05160468],\n",
       "       [0.06486294],\n",
       "       [0.07112415],\n",
       "       [0.07542731]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict([most_likely_RN.squeeze(), np.array([counts_aa(xi) for xi in most_likely_RN.squeeze()])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c7d4b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GYTENV\n",
      "YNTLQF\n",
      "VETYSYA\n",
      "LYYQNV\n",
      "ETLNYTQV\n"
     ]
    }
   ],
   "source": [
    "for vec in most_likely_RN:\n",
    "    print(decoder(vec[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d179bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YWTWMAV\n",
      "KILQWT\n",
      "WMWMY\n",
      "IYWVAL\n",
      "WIRAI\n"
     ]
    }
   ],
   "source": [
    "for vec in least_likely_RN:\n",
    "    print(decoder(vec[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf64827",
   "metadata": {},
   "source": [
    "## Test against PN classifier trained on SHP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c5bbee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('../saved_models/shp2-rnn/PN_model.json', 'r')\n",
    "PN_loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "PN_loaded_model = tf.keras.models.model_from_json(PN_loaded_model_json)\n",
    "# load weights into new model\n",
    "PN_loaded_model.load_weights('../saved_models/shp2-rnn/PN_model_weights.h5') \n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a656d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.6624713 ],\n",
       "       [-2.6087217 ],\n",
       "       [-4.373397  ],\n",
       "       [ 0.87531906],\n",
       "       [-5.268867  ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PN_loaded_model.predict([most_likely_RN.squeeze(), np.array([counts_aa(xi) for xi in most_likely_RN.squeeze()])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83128e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.8970127],\n",
       "       [-3.7263243],\n",
       "       [ 3.0878177],\n",
       "       [ 2.6473737],\n",
       "       [ 1.2609706]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PN_loaded_model.predict([least_likely_RN.squeeze(), np.array([counts_aa(xi) for xi in least_likely_RN.squeeze()])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0309b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "serverless"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
